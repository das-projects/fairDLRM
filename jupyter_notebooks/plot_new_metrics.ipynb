{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metrics with Multiple Features\n",
    "==============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook demonstrates the new API for metrics, which supports\n",
    "multiple sensitive and conditional features. This example does not\n",
    "contain a proper discussion of how fairness relates to the dataset used,\n",
    "although it does highlight issues which users may want to consider when\n",
    "analysing their datasets.\n",
    "\n",
    "We are going to consider a lending scenario, supposing that we have a\n",
    "model which predicts whether or not a particular customer will repay a\n",
    "loan. This could be used as the basis of deciding whether or not to\n",
    "offer that customer a loan. With traditional metrics, we would assess\n",
    "the model using:\n",
    "\n",
    "-   The \\'true\\' values from the test set\n",
    "-   The model predictions from the test set\n",
    "\n",
    "Our fairness metrics compute group-based fairness statistics. To use\n",
    "these, we also need categorical columns from the test set. For this\n",
    "example, we will include:\n",
    "\n",
    "-   The sex of each individual (two unique values)\n",
    "-   The race of each individual (three unique values)\n",
    "-   The credit score band of each individual (three unique values)\n",
    "-   Whether the loan is considered \\'large\\' or \\'small\\'\n",
    "\n",
    "An individual\\'s sex and race should not affect a lending decision, but\n",
    "it would be legitimate to consider an individual\\'s credit score and the\n",
    "relative size of the loan which they desired.\n",
    "\n",
    "A real scenario will be more complicated, but this will serve to\n",
    "illustrate the use of the new metrics.\n",
    "\n",
    "Getting the Data\n",
    "================\n",
    "\n",
    "*This section may be skipped. It simply creates a dataset for\n",
    "illustrative purposes*\n",
    "\n",
    "We will use the well-known UCI \\'Adult\\' dataset as the basis of this\n",
    "demonstration. This is not for a lending scenario, but we will regard it\n",
    "as one for the purposes of this example. We will use the existing\n",
    "\\'race\\' and \\'sex\\' columns (trimming the former to three unique\n",
    "values), and manufacture credit score bands and loan sizes from other\n",
    "columns. We start with some uncontroversial [import]{.title-ref}\n",
    "statements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import selection_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we import the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "X_raw = data.data\n",
    "y = (data.target == '>50K') * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For purposes of clarity, we consolidate the \\'race\\' column to have\n",
    "three unique values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def race_transform(input_str):\n",
    "    \"\"\"Reduce values to White, Black and Other.\"\"\"\n",
    "    result = 'Other'\n",
    "    if input_str == 'White' or input_str == 'Black':\n",
    "        result = input_str\n",
    "    return result\n",
    "\n",
    "\n",
    "X_raw['race'] = X_raw['race'].map(race_transform).fillna('Other').astype('category')\n",
    "print(np.unique(X_raw['race']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we manufacture the columns for the credit score band and requested\n",
    "loan size. These are wholly constructed, and not part of the actual\n",
    "dataset in any way. They are simply for illustrative purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def marriage_transform(m_s_string):\n",
    "    \"\"\"Perform some simple manipulations.\"\"\"\n",
    "    result = 'Low'\n",
    "    if m_s_string.startswith(\"Married\"):\n",
    "        result = 'Medium'\n",
    "    elif m_s_string.startswith(\"Widowed\"):\n",
    "        result = 'High'\n",
    "    return result\n",
    "\n",
    "\n",
    "def occupation_transform(occ_string):\n",
    "    \"\"\"Perform some simple manipulations.\"\"\"\n",
    "    result = 'Small'\n",
    "    if occ_string.startswith(\"Machine\"):\n",
    "        result = 'Large'\n",
    "    return result\n",
    "\n",
    "\n",
    "col_credit = X_raw['marital-status'].map(marriage_transform).fillna('Low')\n",
    "col_credit.name = \"Credit Score\"\n",
    "col_loan_size = X_raw['occupation'].map(occupation_transform).fillna('Small')\n",
    "col_loan_size.name = \"Loan Size\"\n",
    "\n",
    "A = X_raw[['race', 'sex']]\n",
    "A['Credit Score'] = col_credit\n",
    "A['Loan Size'] = col_loan_size\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have imported our dataset and manufactured a few features,\n",
    "we can perform some more conventional processing. To avoid the problem\n",
    "of [data\n",
    "leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)), we\n",
    "need to split the data into training and test sets before applying any\n",
    "transforms or scaling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n",
    "    X_raw, y, A, test_size=0.3, random_state=54321, stratify=y\n",
    ")\n",
    "\n",
    "# Ensure indices are aligned between X, y and A,\n",
    "# after all the slicing and splitting of DataFrames\n",
    "# and Series\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we build two `~sklearn.pipeline.Pipeline`{.interpreted-text\n",
    "role=\"class\"} objects to process the columns, one for numeric data, and\n",
    "the other for categorical data. Both impute missing values; the\n",
    "difference is whether the data are scaled (numeric columns) or one-hot\n",
    "encoded (categorical columns). Imputation of missing values should\n",
    "generally be done with care, since it could potentially introduce\n",
    "biases. Of course, removing rows with missing data could also cause\n",
    "trouble, if particular subgroups have poorer data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our preprocessor defined, we can now build a new pipeline which\n",
    "includes an Estimator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unmitigated_predictor = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(solver=\"liblinear\", fit_intercept=True),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With the pipeline fully defined, we can first train it with the training\n",
    "data, and then generate predictions from the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unmitigated_predictor.fit(X_train, y_train)\n",
    "y_pred = unmitigated_predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Analysing the Model with Metrics\n",
    "================================\n",
    "\n",
    "After our data manipulations and model training, we have the following\n",
    "from our test set:\n",
    "\n",
    "-   A vector of true values called `y_test`\n",
    "-   A vector of model predictions called `y_pred`\n",
    "-   A DataFrame of categorical features relevant to fairness called\n",
    "    `A_test`\n",
    "\n",
    "In a traditional model analysis, we would now look at some metrics\n",
    "evaluated on the entire dataset. Suppose in this case, the relevant\n",
    "metrics are `fairlearn.metrics.selection_rate`{.interpreted-text\n",
    "role=\"func\"} and `sklearn.metrics.fbeta_score`{.interpreted-text\n",
    "role=\"func\"} (with `beta=0.6`). We can evaluate these metrics directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Selection Rate:\", selection_rate(y_test, y_pred))\n",
    "print(\"fbeta:\", skm.fbeta_score(y_test, y_pred, beta=0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We know that there are sensitive features in our data, and we want to\n",
    "ensure that we\\'re not harming individuals due to membership in any of\n",
    "these groups. For this purpose, Fairlearn provides the\n",
    "`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} class.\n",
    "Let us construct an instance of this class, and then look at its\n",
    "capabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fbeta_06 = functools.partial(skm.fbeta_score, beta=0.6)\n",
    "\n",
    "metric_fns = {'selection_rate': selection_rate, 'fbeta_06': fbeta_06}\n",
    "\n",
    "grouped_on_sex = MetricFrame(metric_fns,\n",
    "                             y_test, y_pred,\n",
    "                             sensitive_features=A_test['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}\n",
    "object requires a minimum of four arguments:\n",
    "\n",
    "1.  The underlying metric function(s) to be evaluated\n",
    "2.  The true values\n",
    "3.  The predicted values\n",
    "4.  The sensitive feature values\n",
    "\n",
    "These are all passed as arguments to the constructor. If more than one\n",
    "underlying metric is required (as in this case), then we must provide\n",
    "them in a dictionary.\n",
    "\n",
    "The underlying metrics must have a signature `fn(y_true, y_pred)`, so we\n",
    "have to use `functools.partial`{.interpreted-text role=\"func\"} on\n",
    "`fbeta_score()` to furnish `beta=0.6` (we will show how to pass in extra\n",
    "array arguments such as sample weights shortly).\n",
    "\n",
    "We will now take a closer look at the\n",
    "`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} object.\n",
    "First, there is the `overall` property, which contains the metrics\n",
    "evaluated on the entire dataset. We see that this contains the same\n",
    "values calculated above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert grouped_on_sex.overall['selection_rate'] == selection_rate(y_test, y_pred)\n",
    "assert grouped_on_sex.overall['fbeta_06'] == skm.fbeta_score(y_test, y_pred, beta=0.6)\n",
    "print(grouped_on_sex.overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The other property in the\n",
    "`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} object\n",
    "is `by_group`. This contains the metrics evaluated on each subgroup\n",
    "defined by the categories in the `sensitive_features=` argument. In this\n",
    "case, we have results for males and females:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_sex.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can immediately see a substantial disparity in the selection rate\n",
    "between males and females.\n",
    "\n",
    "We can also create another\n",
    "`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"} object\n",
    "using race as the sensitive feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race = MetricFrame(metric_fns,\n",
    "                              y_test, y_pred,\n",
    "                              sensitive_features=A_test['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `overall` property is unchanged:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert (grouped_on_sex.overall == grouped_on_race.overall).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `by_group` property now contains the metrics evaluated based on the\n",
    "\\'race\\' column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that there is also a significant disparity in selection rates\n",
    "when grouping by race.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sample weights and other arrays\n",
    "===============================\n",
    "\n",
    "We noted above that the underlying metric functions passed to the\n",
    "`fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}\n",
    "constructor need to be of the form `fn(y_true, y_pred)` - we do not\n",
    "support scalar arguments such as `pos_label=` or `beta=` in the\n",
    "constructor. Such arguments should be bound into a new function using\n",
    "`functools.partial`{.interpreted-text role=\"func\"}, and the result\n",
    "passed in. However, we do support arguments which have one entry for\n",
    "each sample, with an array of sample weights being the most common\n",
    "example. These are divided into subgroups along with `y_true` and\n",
    "`y_pred`, and passed along to the underlying metric.\n",
    "\n",
    "To use these arguments, we pass in a dictionary as the `sample_params=`\n",
    "argument of the constructor. Let us generate some random weights, and\n",
    "pass these along:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_weights = np.random.rand(len(y_test))\n",
    "\n",
    "example_sample_params = {\n",
    "    'selection_rate': {'sample_weight': random_weights},\n",
    "    'fbeta_06': {'sample_weight': random_weights},\n",
    "}\n",
    "\n",
    "\n",
    "grouped_with_weights = MetricFrame(metric_fns,\n",
    "                                   y_test, y_pred,\n",
    "                                   sensitive_features=A_test['sex'],\n",
    "                                   sample_params=example_sample_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can inspect the overall values, and check they are as expected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert grouped_with_weights.overall['selection_rate'] == \\\n",
    "    selection_rate(y_test, y_pred, sample_weight=random_weights)\n",
    "assert grouped_with_weights.overall['fbeta_06'] == \\\n",
    "    skm.fbeta_score(y_test, y_pred, beta=0.6, sample_weight=random_weights)\n",
    "print(grouped_with_weights.overall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also see the effect on the metric being evaluated on the\n",
    "subgroups:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_with_weights.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Quantifying Disparities\n",
    "=======================\n",
    "\n",
    "We now know that our model is selecting individuals who are female far\n",
    "less often than individuals who are male. There is a similar effect when\n",
    "examining the results by race, with blacks being selected far less often\n",
    "than whites (and those classified as \\'other\\'). However, there are many\n",
    "cases where presenting all these numbers at once will not be useful (for\n",
    "example, a high level dashboard which is monitoring model performance).\n",
    "Fairlearn provides several means of aggregating metrics across the\n",
    "subgroups, so that disparities can be readily quantified.\n",
    "\n",
    "The simplest of these aggregations is `group_min()`, which reports the\n",
    "minimum value seen for a subgroup for each underlying metric (we also\n",
    "provide `group_max()`). This is useful if there is a mandate that \\\"no\n",
    "subgroup should have an `fbeta_score()` of less than 0.6.\\\" We can\n",
    "evaluate the minimum values easily:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race.group_min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As noted above, the selection rates varies greatly by race and by sex.\n",
    "This can be quantified in terms of a difference between the subgroup\n",
    "with the highest value of the metric, and the subgroup with the lowest\n",
    "value. For this, we provide the method\n",
    "`difference(method='between_groups)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race.difference(method='between_groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also evaluate the difference relative to the corresponding\n",
    "overall value of the metric. In this case we take the absolute value, so\n",
    "that the result is always positive:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race.difference(method='to_overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are situations where knowing the ratios of the metrics evaluated\n",
    "on the subgroups is more useful. For this we have the `ratio()` method.\n",
    "We can take the ratios between the minimum and maximum values of each\n",
    "metric:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race.ratio(method='between_groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also compute the ratios relative to the overall value for each\n",
    "metric. Analogous to the differences, the ratios are always in the range\n",
    "$[0,1]$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race.ratio(method='to_overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Intersections of Features\n",
    "=========================\n",
    "\n",
    "So far we have only considered a single sensitive feature at a time, and\n",
    "we have already found some serious issues in our example data. However,\n",
    "sometimes serious issues can be hiding in intersections of features. For\n",
    "example, the [Gender Shades\n",
    "project](https://www.media.mit.edu/projects/gender-shades/overview/)\n",
    "found that facial recognition algorithms performed worse for blacks than\n",
    "whites, and also worse for women than men (despite overall high accuracy\n",
    "score). Moreover, performance on black females was *terrible*. We can\n",
    "examine the intersections of sensitive features by passing multiple\n",
    "columns to the `fairlearn.metrics.MetricFrame`{.interpreted-text\n",
    "role=\"class\"} constructor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race_and_sex = MetricFrame(metric_fns,\n",
    "                                      y_test, y_pred,\n",
    "                                      sensitive_features=A_test[['race', 'sex']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The overall values are unchanged, but the `by_group` table now shows the\n",
    "intersections between subgroups:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert (grouped_on_race_and_sex.overall == grouped_on_race.overall).all()\n",
    "grouped_on_race_and_sex.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The aggregations are still performed across all subgroups for each\n",
    "metric, so each continues to reduce to a single value. If we look at the\n",
    "`group_min()`, we see that we violate the mandate we specified for the\n",
    "`fbeta_score()` suggested above (for females with a race of \\'Other\\' in\n",
    "fact):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race_and_sex.group_min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at the `ratio()` method, we see that the disparity is worse\n",
    "(specifically between white males and black females, if we check in the\n",
    "`by_group` table):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grouped_on_race_and_sex.ratio(method='between_groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Control Features\n",
    "================\n",
    "\n",
    "There is a further way we can slice up our data. We have (*completely\n",
    "made up*) features for the individuals\\' credit scores (in three bands)\n",
    "and also the size of the loan requested (large or small). In our loan\n",
    "scenario, it is acceptable that individuals with high credit scores are\n",
    "selected more often than individuals with low credit scores. However,\n",
    "within each credit score band, we do not want a disparity between (say)\n",
    "black females and white males. To example these cases, we have the\n",
    "concept of *control features*.\n",
    "\n",
    "Control features are introduced by the `control_features=` argument to\n",
    "the `fairlearn.metrics.MetricFrame`{.interpreted-text role=\"class\"}\n",
    "object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_credit_score = MetricFrame(metric_fns,\n",
    "                                y_test, y_pred,\n",
    "                                sensitive_features=A_test[['race', 'sex']],\n",
    "                                control_features=A_test['Credit Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This has an immediate effect on the `overall` property. Instead of\n",
    "having one value for each metric, we now have a value for each unique\n",
    "value of the control feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_credit_score.overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `by_group` property is similarly expanded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_credit_score.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The aggregates are also evaluated once for each group identified by the\n",
    "control feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_credit_score.group_min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_credit_score.ratio(method='between_groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In our data, we see that we have a dearth of positive results for high\n",
    "income non-whites, which significantly affects the aggregates.\n",
    "\n",
    "We can continue adding more control features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_both = MetricFrame(metric_fns,\n",
    "                        y_test, y_pred,\n",
    "                        sensitive_features=A_test[['race', 'sex']],\n",
    "                        control_features=A_test[['Loan Size', 'Credit Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `overall` property now splits into more values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_both.overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As does the `by_groups` property, where `NaN` values indicate that there\n",
    "were no samples in the cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cond_both.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The aggregates behave similarly. By this point, we are having\n",
    "significant issues with under-populated intersections. Consider:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def member_counts(y_true, y_pred):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    return len(y_true)\n",
    "\n",
    "\n",
    "counts = MetricFrame(member_counts,\n",
    "                     y_test, y_pred,\n",
    "                     sensitive_features=A_test[['race', 'sex']],\n",
    "                     control_features=A_test[['Loan Size', 'Credit Score']])\n",
    "\n",
    "counts.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall that `NaN` indicates that there were no individuals in a cell -\n",
    "`member_counts()` will not even have been called.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}